# https://docs.vllm.ai/projects/recipes/en/latest/OpenAI/GPT-OSS.html#prepare-the-config-file
kv-cache-dtype: fp8
compilation-config: '{"pass_config":{"enable_fi_allreduce_fusion":true,"enable_noop":true}}'
async-scheduling: true
no-enable-prefix-caching: true
max-num-batched-tokens: 8192

# BUG: not sure why unrecognized args?
# max-cudagraph-capture-size: 2048
# stream-interval: 20